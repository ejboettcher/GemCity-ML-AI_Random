{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMoOTFWoFP1m"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ejboettcher/GemCity-ML-AI_Random/blob/master/Random_Num_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywktc8UkWbRv"
      },
      "source": [
        "# Random Number Testing\n",
        "\n",
        "I was recently going through [Deeplearning-AI worksheet on fitting Sun Spot data with timeseries](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C4/W4/ungraded_labs/C4_W4_Lab_3_Sunspots_CNN_RNN_DNN.ipynb).  I was shocked that they wanted to fit to the \"noise\" and where getting so close of a fit with what looked like \"random\" noise. That got me thinking, can I use TensorFlow to see if the random number generator that I am using is truely random.  Let play.\n",
        "\n",
        "First lets walk through the sunspots tutorial.  Then we can play with numpy's and python's random generator.\n",
        "\n",
        "# Deeplearning-AI Lab: Predicting Sunspots with Neural Networks\n",
        "\n",
        " In this lab, you'll try one more configuration and that is a combination of DNNs, RNNs, and CNNs types of networks: the data windows will pass through a convolution, followed by stacked LSTMs, followed by stacked dense layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sun Spot Tutorial"
      ],
      "metadata": {
        "id": "MK0F3fWpEBS3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR3s1iNXW8qv"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLl52leVp5wU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTgnwTJiW-bF"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDrgVqCTao9j"
      },
      "outputs": [],
      "source": [
        "def plot_series(x, y, format=\"-\", start=0, end=None, \n",
        "                title=None, xlabel=None, ylabel=None, legend=None ):\n",
        "    \"\"\"\n",
        "    Visualizes time series data\n",
        "\n",
        "    Args:\n",
        "      x (array of int) - contains values for the x-axis\n",
        "      y (array of int or tuple of arrays) - contains the values for the y-axis\n",
        "      format (string) - line style when plotting the graph\n",
        "      start (int) - first time step to plot\n",
        "      end (int) - last time step to plot\n",
        "      title (string) - title of the plot\n",
        "      xlabel (string) - label for the x-axis\n",
        "      ylabel (string) - label for the y-axis\n",
        "      legend (list of strings) - legend for the plot\n",
        "    \"\"\"\n",
        "\n",
        "    # Setup dimensions of the graph figure\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Check if there are more than two series to plot\n",
        "    if type(y) is tuple:\n",
        "      # Loop over the y elements\n",
        "      for y_curr in y:\n",
        "        # Plot the x and current y values\n",
        "        plt.plot(x[start:end], y_curr[start:end], format)\n",
        "\n",
        "    else:\n",
        "      # Plot the x and y values\n",
        "      plt.plot(x[start:end], y[start:end], format)\n",
        "\n",
        "    # Labels\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    # Set the legend\n",
        "    if legend:\n",
        "      plt.legend(legend)\n",
        "\n",
        "    # Overlay a grid on the graph\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Draw the graph on screen\n",
        "    plt.show()\n",
        "\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    \"\"\"Generates dataset windows\n",
        "\n",
        "    Args:\n",
        "      series (array of float) - contains the values of the time series\n",
        "      window_size (int) - the number of time steps to include in the feature\n",
        "      batch_size (int) - the batch size\n",
        "      shuffle_buffer(int) - buffer size to use for the shuffle method\n",
        "\n",
        "    Returns:\n",
        "      dataset (TF Dataset) - TF Dataset containing time windows\n",
        "    \"\"\"\n",
        "  \n",
        "    # Generate a TF Dataset from the series values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "    \n",
        "    # Window the data but only take those with the specified size\n",
        "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    \n",
        "    # Flatten the windows by putting its elements in a single batch\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "\n",
        "    # Create tuples with features and labels \n",
        "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
        "\n",
        "    # Shuffle the windows\n",
        "    dataset = dataset.shuffle(shuffle_buffer)\n",
        "    \n",
        "    # Create batches of windows\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU9FU6-BXE56"
      },
      "source": [
        "## Download and Preview the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miE0cEqdu3eS"
      },
      "outputs": [],
      "source": [
        "# Download the Dataset\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course4/Sunspots.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcG9r1eClbTh"
      },
      "outputs": [],
      "source": [
        "# Initialize lists\n",
        "time_step = []\n",
        "sunspots = []\n",
        "\n",
        "# Open CSV file\n",
        "with open('./Sunspots.csv') as csvfile:\n",
        "  \n",
        "  # Initialize reader\n",
        "  reader = csv.reader(csvfile, delimiter=',')\n",
        "  \n",
        "  # Skip the first line\n",
        "  next(reader)\n",
        "  \n",
        "  # Append row and sunspot number to lists\n",
        "  for row in reader:\n",
        "    time_step.append(int(row[0]))\n",
        "    sunspots.append(float(row[2]))\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "time = np.array(time_step)\n",
        "series = np.array(sunspots)\n",
        "\n",
        "# Preview the data\n",
        "plot_series(time, series, xlabel='Month', ylabel='Monthly Mean Total Sunspot Number')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq34m86mXHk4"
      },
      "source": [
        "## Split the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L92YRw_IpCFG"
      },
      "outputs": [],
      "source": [
        "# Define the split time\n",
        "split_time = 3000\n",
        "\n",
        "# Get the train set \n",
        "time_train = time[:split_time]\n",
        "x_train = series[:split_time]\n",
        "\n",
        "# Get the validation set\n",
        "time_valid = time[split_time:]\n",
        "x_valid = series[split_time:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlJDtNMDXkJF"
      },
      "source": [
        "## Prepare Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJwUUZscnG38"
      },
      "outputs": [],
      "source": [
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    \"\"\"Generates dataset windows\n",
        "\n",
        "    Args:\n",
        "      series (array of float) - contains the values of the time series\n",
        "      window_size (int) - the number of time steps to include in the feature\n",
        "      batch_size (int) - the batch size\n",
        "      shuffle_buffer(int) - buffer size to use for the shuffle method\n",
        "\n",
        "    Returns:\n",
        "      dataset (TF Dataset) - TF Dataset containing time windows\n",
        "    \"\"\"\n",
        "  \n",
        "    # Generate a TF Dataset from the series values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "    \n",
        "    # Window the data but only take those with the specified size\n",
        "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    \n",
        "    # Flatten the windows by putting its elements in a single batch\n",
        "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
        "\n",
        "    # Create tuples with features and labels \n",
        "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
        "\n",
        "    # Shuffle the windows\n",
        "    dataset = dataset.shuffle(shuffle_buffer)\n",
        "    \n",
        "    # Create batches of windows\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lakVlpDry5iv"
      },
      "source": [
        "As mentioned in the lectures, if your results don't good, you can try tweaking the parameters here and see if the model will learn better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z0qRGp1zl5b"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "window_size = 30\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000\n",
        "\n",
        "# Generate the dataset windows\n",
        "train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylQoAXTaXscV"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "You've seen these layers before and here is how it's looks like when combined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6kJd40-0Hj9"
      },
      "outputs": [],
      "source": [
        "# Build the Model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv1D(filters=64, kernel_size=3,\n",
        "                      strides=1,\n",
        "                      activation=\"relu\",\n",
        "                      padding='causal',\n",
        "                      input_shape=[window_size, 1]),\n",
        "  tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "  tf.keras.layers.LSTM(64),\n",
        "  tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(1),\n",
        "  tf.keras.layers.Lambda(lambda x: x * 400)\n",
        "])\n",
        "\n",
        " # Print the model summary \n",
        "model.summary()\n",
        "\n",
        "# Get initial weights\n",
        "init_weights =model.get_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-Pn-n60X3uV"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "Now you can proceed to reset and train the model. It is set for 100 epochs in the cell below but feel free to increase it if you want. Laurence got his results in the lectures after 500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsksvkcXAAgq"
      },
      "outputs": [],
      "source": [
        "# Reset states generated by Keras\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Reset the weights\n",
        "model.set_weights(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBYFogmdFM-R"
      },
      "outputs": [],
      "source": [
        "# Set the initial learning rate\n",
        "initial_learning_rate=1e-7\n",
        "\n",
        "# Define the scheduler\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=400,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "# Set the optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
        "\n",
        "# Set the training parameters\n",
        "model.compile(loss=tf.keras.losses.Huber(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"mae\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrcqLyLALmCY"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(train_set,epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAzIkJd0L6WD"
      },
      "source": [
        "You can visualize the training and see if the loss and MAE are still trending down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD2kyYUVt3O0"
      },
      "outputs": [],
      "source": [
        "# Get mae and loss from history log\n",
        "mae=history.history['mae']\n",
        "loss=history.history['loss']\n",
        "\n",
        "# Get number of epochs\n",
        "epochs=range(len(loss)) \n",
        "\n",
        "# Plot mae and loss\n",
        "plot_series(\n",
        "    x=epochs, \n",
        "    y=(mae, loss), \n",
        "    title='MAE and Loss', \n",
        "    xlabel='MAE',\n",
        "    ylabel='Loss',\n",
        "    legend=['MAE', 'Loss']\n",
        "    )\n",
        "\n",
        "# Only plot the last 80% of the epochs\n",
        "zoom_split = int(epochs[-1] * 0.2)\n",
        "epochs_zoom = epochs[zoom_split:]\n",
        "mae_zoom = mae[zoom_split:]\n",
        "loss_zoom = loss[zoom_split:]\n",
        "\n",
        "# Plot zoomed mae and loss\n",
        "plot_series(\n",
        "    x=epochs_zoom, \n",
        "    y=(mae_zoom, loss_zoom), \n",
        "    title='MAE and Loss', \n",
        "    xlabel='MAE',\n",
        "    ylabel='Loss',\n",
        "    legend=['MAE', 'Loss']\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHxos0eEX7b7"
      },
      "source": [
        "## Model Prediction\n",
        "\n",
        "As before, you can get the predictions for the validation set time range and compute the metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XwGrf-A_wF0"
      },
      "outputs": [],
      "source": [
        "def model_forecast(model, series, window_size, batch_size):\n",
        "    \"\"\"Uses an input model to generate predictions on data windows\n",
        "\n",
        "    Args:\n",
        "      model (TF Keras Model) - model that accepts data windows\n",
        "      series (array of float) - contains the values of the time series\n",
        "      window_size (int) - the number of time steps to include in the window\n",
        "      batch_size (int) - the batch size\n",
        "\n",
        "    Returns:\n",
        "      forecast (numpy array) - array containing predictions\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a TF Dataset from the series values\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
        "\n",
        "    # Window the data but only take those with the specified size\n",
        "    dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n",
        "\n",
        "    # Flatten the windows by putting its elements in a single batch\n",
        "    dataset = dataset.flat_map(lambda w: w.batch(window_size))\n",
        "    \n",
        "    # Create batches of windows\n",
        "    dataset = dataset.batch(batch_size).prefetch(1)\n",
        "    \n",
        "    # Get predictions on the entire dataset\n",
        "    forecast = model.predict(dataset)\n",
        "    \n",
        "    return forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5f7kKbFL6zv"
      },
      "outputs": [],
      "source": [
        "# Reduce the original series\n",
        "forecast_series = series[split_time-window_size:-1]\n",
        "\n",
        "# Use helper function to generate predictions\n",
        "forecast = model_forecast(model, forecast_series, window_size, batch_size)\n",
        "\n",
        "# Drop single dimensional axis\n",
        "results = forecast.squeeze()\n",
        "\n",
        "# Plot the results\n",
        "plot_series(time_valid, (x_valid, results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jZrRYjQMNHr"
      },
      "outputs": [],
      "source": [
        "# Compute the MAE\n",
        "print(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Numbers\n",
        "\n",
        "Python Random Library [from python docs](https://docs.python.org/3/library/random.html): generates a random float uniformly in the semi-open range [0.0, 1.0). Python uses the Mersenne Twister as the core generator. It produces 53-bit precision floats and has a period of 2**19937-1. The underlying implementation in C is both fast and threadsafe. The Mersenne Twister is one of the most extensively tested random number generators in existence. However, being completely deterministic, it is not suitable for all purposes, and is completely unsuitable for cryptographic purposes.\n",
        "\n",
        "## Fitting to pseudo-random\n",
        "Let's generate a 10,000 series of random numbers.\n",
        "\n",
        "First we will try \"mae\": Mean Abosolute Error, then we will try \"accuracy\" as our metric.\n"
      ],
      "metadata": {
        "id": "pz016ekcKYdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pseudoRandomSeries = [random.randint(0,100) for ii in range(100_000)]\n",
        "print(\"Size\", len(pseudoRandomSeries))\n",
        "print(\"Top ten\", pseudoRandomSeries[:10])\n",
        "\n"
      ],
      "metadata": {
        "id": "E1AawS7ELlnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the Pseudo Random Dataset"
      ],
      "metadata": {
        "id": "OTuGPg-FykBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the pseudo-random dataset\n",
        "tf.keras.backend.clear_session()\n",
        "# Parameters\n",
        "window_size = 50\n",
        "batch_size = 100\n",
        "shuffle_buffer_size = 500\n",
        "split_time = 99_000\n",
        "# Train dataset\n",
        "x_train = pseudoRandomSeries[:split_time]\n",
        "\n",
        "# validation dataset\n",
        "x_valid = pseudoRandomSeries[split_time:]\n",
        "# Generate the dataset windows\n",
        "train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "\n",
        "# Build the Model\n",
        "pseudo_model = tf.keras.models.Sequential([\n",
        " #tf.keras.layers.Conv1D(filters=32, kernel_size=3,\n",
        " #                     strides=1,\n",
        " #                     activation=\"relu\",\n",
        " #                     padding='causal',\n",
        " #                     input_shape=[window_size, 1]),\n",
        " tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                          input_shape=[window_size]),\n",
        "  \n",
        "      tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "      tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "      tf.keras.layers.LSTM(64),\n",
        "      tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(1),\n",
        "      tf.keras.layers.Lambda(lambda x: x * 100)\n",
        "])\n",
        "pseudo_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ysoTKjwUYNgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Learning Rate \n",
        "Skipping this since I already ran it. "
      ],
      "metadata": {
        "id": "7i9PihLDnCzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the learning rate scheduler\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n",
        "\n",
        "# Set the training parameters\n",
        "pseudo_model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer)\n",
        "\n",
        "# Train the model\n",
        "history = pseudo_model.fit(train_set, epochs=100, callbacks=[lr_schedule])"
      ],
      "metadata": {
        "id": "ARV-HK0AlgO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the learning rate array\n",
        "lrs = 1e-8 * (10 ** (np.arange(100) / 20))\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.grid(True)  # Set the grid\n",
        "\n",
        "# Plot the loss in log scale\n",
        "plt.semilogx(lrs, history.history[\"loss\"])\n",
        "\n",
        "# Increase the tickmarks size\n",
        "plt.tick_params('both', length=10, width=1, which='both')\n",
        "plt.axis([1e-8, 1e-3, 20, 36.5]) # Set the plot boundaries\n"
      ],
      "metadata": {
        "id": "4QoyHmJAloKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "VS3O0JKunPM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the initial learning rate\n",
        "initial_learning_rate = 5e-6\n",
        "\n",
        "# We are going to run this with two different  metrics: mae and accuracy\n",
        "metrics = \"mae\"  # \"accuracy\"\n",
        "\n",
        "# Define the scheduler\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=400,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "# Set the optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
        "\n",
        "# Set the training parameters\n",
        "pseudo_model.compile( loss=tf.keras.losses.Huber(),\n",
        "              #loss=tf.keras.losses.MeanAbsoluteError(),\n",
        "              optimizer=optimizer,\n",
        "              metrics=[metrics])\n",
        "\n"
      ],
      "metadata": {
        "id": "ntxJS_2Ulf1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model\n",
        "history = pseudo_model.fit(train_set, epochs=40)"
      ],
      "metadata": {
        "id": "CO6KCx0warvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the loss history \n",
        "# Get mae and loss from history log\n",
        "mae = history.history[metrics]\n",
        "loss = history.history['loss']\n",
        "\n",
        "# Get number of epochs\n",
        "epochs = range(len(loss)) \n",
        "\n",
        "# Plot mae and loss\n",
        "plot_series(\n",
        "    x=epochs, \n",
        "    y=(mae, loss), \n",
        "    title= metrics.title() +'and Loss', \n",
        "    xlabel=metrics.title(),\n",
        "    ylabel='Loss',\n",
        "    legend=[metrics.title(), 'Loss']\n",
        "    )"
      ],
      "metadata": {
        "id": "dE3Mb_rEjreM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forcast: Visual Validatation"
      ],
      "metadata": {
        "id": "-zjafWExFed7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forcast\n",
        "forecast_series = pseudoRandomSeries[split_time-window_size:-1]\n",
        "# Use helper function to generate predictions\n",
        "forecast = model_forecast(pseudo_model, forecast_series, window_size, batch_size)\n",
        "\n",
        "# Drop single dimensional axis\n",
        "results = forecast.squeeze()\n",
        "\n",
        "time_valid = list(range(len(results)))\n",
        "# Plot the results\n",
        "plot_series(time_valid, (x_valid, results))\n",
        "\n"
      ],
      "metadata": {
        "id": "8G5fi990heYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoTlB3Go0sRf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Only plot the last 10% of the epochs\n",
        "zoom_split = 100\n",
        "time_zoom = time_valid[-zoom_split:]\n",
        "x_zoom = x_valid[-zoom_split:]\n",
        "results_zoom = results[-zoom_split:]\n",
        "\n",
        "# Plot zoomed mae and loss\n",
        "plot_series(\n",
        "    x=time_zoom, \n",
        "    y=(x_zoom, results_zoom), \n",
        "    title='Results', \n",
        "    xlabel='Time',\n",
        "    ylabel='Random Int',\n",
        "    legend=['X', 'Results']\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results[:10])\n"
      ],
      "metadata": {
        "id": "twIDEF0WivOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ideas for next week?\n",
        "\n",
        "* Hot Encoding to see if things improve.  \n",
        "   * Right now TF is trying to fit floats instead of ints.  So if we hot encode it will forst TF to fit floats.\n",
        "* Test if we can find a pattern in a cryto message\n",
        "   * A good cryto algorithm tries to make the message look like \"random\", can we use TF to see if it finds a pattern as a test for Noise/message\n",
        "* Use a more Pseudo generator\n",
        "* Something else? \n"
      ],
      "metadata": {
        "id": "RwfaWTM_tzdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hot Encoding: Generate Series"
      ],
      "metadata": {
        "id": "OBPlSbFPDQ2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "pseudoRandomSeries = np.zeros((10_000, 100))\n",
        "\n",
        "for ii in range( 10_000):\n",
        "    index = random.randint(0, 99)\n",
        "    pseudoRandomSeries[ii, index] = 1\n",
        "print(\"Size\", len(pseudoRandomSeries))\n",
        "print(\"Top ten\", pseudoRandomSeries[:10])\n"
      ],
      "metadata": {
        "id": "5tJJ119FuDi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Need a new model for hot encoding.\n"
      ],
      "metadata": {
        "id": "I2AiPdMUuMDA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSOyUJaTuRxm"
      },
      "outputs": [],
      "source": [
        "# Build the Model\n",
        "\n",
        "pseudo_model = tf.keras.models.Sequential([\n",
        " #tf.keras.layers.Conv1D(filters=32, kernel_size=3,\n",
        " #                     strides=1,\n",
        " #                     activation=\"relu\",\n",
        " #                     padding='causal',\n",
        " #                     input_shape=[window_size, 1]),\n",
        " tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
        "                          input_shape=[50, 100]),\n",
        "  tf.keras.layers.LSTM(32, return_sequences=True),\n",
        "  tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "  tf.keras.layers.LSTM(64),\n",
        "  tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "  # Changed to 100 from 1\n",
        "  tf.keras.layers.Dense(100, activation\"relu\"),]) \n",
        "  # tf.keras.layers.Lambda(lambda x: x * 100)\n",
        "\n",
        "pseudo_model.summary()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Random-Num-Testing.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
